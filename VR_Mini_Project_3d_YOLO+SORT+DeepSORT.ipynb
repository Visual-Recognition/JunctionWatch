{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"159dbaaacd864d069a5c9b8f9838c3e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24eca512f43d4192b2c7d205c3e3bbf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_808db17d278548d287e5205f0da93990","IPY_MODEL_39d7285e582a4d0ba8d403ccf62467c4","IPY_MODEL_ddaa0bbc168a41b1aa687f157d800aa2"],"layout":"IPY_MODEL_54b3b2699b384f08aabd2fdbac992605"}},"39d7285e582a4d0ba8d403ccf62467c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3bf5a07f6f34b2cb6ee9ce889dfcd2a","max":175221657,"min":0,"orientation":"horizontal","style":"IPY_MODEL_599da8fd042e4d50b85bc84c1e6c260d","value":175221657}},"443d50f145b24a5481dfc3e8d8de27f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54b3b2699b384f08aabd2fdbac992605":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"599da8fd042e4d50b85bc84c1e6c260d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"808db17d278548d287e5205f0da93990":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_443d50f145b24a5481dfc3e8d8de27f1","placeholder":"​","style":"IPY_MODEL_df130147bfb34ec3a61519373e17be60","value":"100%"}},"92880adc5a0e41c1b2d577cb46ab2e7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddaa0bbc168a41b1aa687f157d800aa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_159dbaaacd864d069a5c9b8f9838c3e1","placeholder":"​","style":"IPY_MODEL_92880adc5a0e41c1b2d577cb46ab2e7c","value":" 167M/167M [00:00&lt;00:00, 232MB/s]"}},"df130147bfb34ec3a61519373e17be60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3bf5a07f6f34b2cb6ee9ce889dfcd2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7864106,"sourceType":"datasetVersion","datasetId":4613459},{"sourceId":7903979,"sourceType":"datasetVersion","datasetId":4622438}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Visual Recognition Assignment 3 part d\n \n","metadata":{"id":"XRtfCCUOvbrD"}},{"cell_type":"code","source":"import torch \nimport torchvision \nimport cv2 \nfrom PIL import Image \nfrom torchvision import transforms as T \nimport numpy as np \nfrom torchvision import transforms\nfrom torch import device\nfrom torch import cuda\nimport time \nimport shutil\nimport git\nimport sys\n\nfrom sort import *\nfrom deep_sort_realtime.deepsort_tracker import DeepSort ","metadata":{"id":"RHkjjEmNvYcd","execution":{"iopub.status.busy":"2024-03-22T13:56:24.712739Z","iopub.execute_input":"2024-03-22T13:56:24.713007Z","iopub.status.idle":"2024-03-22T13:56:24.717981Z","shell.execute_reply.started":"2024-03-22T13:56:24.712983Z","shell.execute_reply":"2024-03-22T13:56:24.716994Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"git.Repo.clone_from('https://github.com/KalyanRam1234/sort.git', '/kaggle/working/sample1')\nsys.path.insert(0,'/kaggle/working/sample1/')\n\n#Uncomment like below if running it for 2nd time or more --------------------------------------------\n\n# shutil.rmtree('/kaggle/working/sample1')","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:56:24.743383Z","iopub.execute_input":"2024-03-22T13:56:24.743715Z","iopub.status.idle":"2024-03-22T13:56:24.748388Z","shell.execute_reply.started":"2024-03-22T13:56:24.743689Z","shell.execute_reply":"2024-03-22T13:56:24.747368Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"!pip install filterpy==1.4.5\n!pip install lap==0.4.0","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:56:24.764421Z","iopub.execute_input":"2024-03-22T13:56:24.764721Z","iopub.status.idle":"2024-03-22T13:56:49.464134Z","shell.execute_reply.started":"2024-03-22T13:56:24.764698Z","shell.execute_reply":"2024-03-22T13:56:49.462990Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: filterpy==1.4.5 in /opt/conda/lib/python3.10/site-packages (1.4.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from filterpy==1.4.5) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from filterpy==1.4.5) (1.11.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from filterpy==1.4.5) (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->filterpy==1.4.5) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->filterpy==1.4.5) (1.16.0)\nRequirement already satisfied: lap==0.4.0 in /opt/conda/lib/python3.10/site-packages (0.4.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##### We will use torch's own pretrained implementation as training an RCNN takes substantial amount of time. This implementation is made with ResNet 50 backbone and trained on MS Coco. ","metadata":{"id":"l8aVMf5iydZv"}},{"cell_type":"code","source":"device = device('cuda:0' if cuda.is_available() else 'cpu')","metadata":{"id":"LLoTP0Ssf4Wm","execution":{"iopub.status.busy":"2024-03-22T13:56:49.474872Z","iopub.execute_input":"2024-03-22T13:56:49.475321Z","iopub.status.idle":"2024-03-22T13:56:49.537652Z","shell.execute_reply.started":"2024-03-22T13:56:49.475288Z","shell.execute_reply":"2024-03-22T13:56:49.536681Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#### YOLO v5 \nWe used YOLOv5x for the purpose of this classification. A pre-trained model was available on pytorch.","metadata":{"id":"OmLu48NFf4Wp"}},{"cell_type":"code","source":"%pip install  -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt  # install dependencies","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLLaubWTf4Wp","outputId":"b0ce972e-4148-4f4c-e650-081ca4e6c5ef","execution":{"iopub.status.busy":"2024-03-22T13:56:49.552217Z","iopub.execute_input":"2024-03-22T13:56:49.552992Z","iopub.status.idle":"2024-03-22T13:57:03.875566Z","shell.execute_reply.started":"2024-03-22T13:56:49.552943Z","shell.execute_reply":"2024-03-22T13:57:03.874430Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True).to(device)","metadata":{"id":"Z7fiVB32f4Wp","execution":{"iopub.status.busy":"2024-03-22T13:57:03.876996Z","iopub.execute_input":"2024-03-22T13:57:03.877343Z","iopub.status.idle":"2024-03-22T13:57:10.436580Z","shell.execute_reply.started":"2024-03-22T13:57:03.877312Z","shell.execute_reply":"2024-03-22T13:57:10.435864Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n  warnings.warn(\nDownloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\nYOLOv5 🚀 2024-3-22 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt to yolov5x.pt...\n100%|██████████| 166M/166M [00:00<00:00, 296MB/s] \n\nFusing layers... \nYOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\nAdding AutoShape... \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### DeepSort and SORT","metadata":{"id":"2g5y1jxff4Wr"}},{"cell_type":"code","source":"!pip install deep-sort-realtime","metadata":{"id":"uazkonZ_f4Wr","execution":{"iopub.status.busy":"2024-03-22T13:57:10.437723Z","iopub.execute_input":"2024-03-22T13:57:10.438302Z","iopub.status.idle":"2024-03-22T13:57:23.428397Z","shell.execute_reply.started":"2024-03-22T13:57:10.438273Z","shell.execute_reply":"2024-03-22T13:57:23.427155Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Collecting deep-sort-realtime\n  Using cached deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deep-sort-realtime) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from deep-sort-realtime) (1.11.4)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from deep-sort-realtime) (4.9.0.80)\nUsing cached deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\nInstalling collected packages: deep-sort-realtime\nSuccessfully installed deep-sort-realtime-1.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#SORT\nobject_tracker = Sort(max_age=600, min_hits=2)\n\n#DeepSORT object tracker, uncomment to use ------------------------------------------\n\n# object_tracker = DeepSort(max_iou_distance=0.2,max_age=20,nms_max_overlap = 0.5,gating_only_position=True,n_init=2,max_cosine_distance=0.9)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:24.219037Z","iopub.execute_input":"2024-03-22T13:57:24.219342Z","iopub.status.idle":"2024-03-22T13:57:24.223576Z","shell.execute_reply.started":"2024-03-22T13:57:24.219316Z","shell.execute_reply":"2024-03-22T13:57:24.222635Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Yolo v5 + DeepSort","metadata":{"id":"WDXmq58of4Wr"}},{"cell_type":"code","source":"def TrackCarSort(model, video, car_index, object_tracker, out_path):\n    model.eval()\n    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), 15, (width, height))\n    \n    setIDs = set()\n    j = 0\n    while video.isOpened():\n        ret, frame = video.read()   #frames are read one by one\n        if not ret:\n            break\n            \n        with torch.no_grad():\n            pred = model(frame)\n            pred = pred.xyxy[0]  #take only the first element of the model output, pred, which is used ahead\n\n        detections = []\n        \n        for detection in pred:\n            class_id = int(detection[5])\n            confidence = detection[4]\n            if confidence > 0.5 and class_id == car_index:\n                x_min, y_min, x_max, y_max = detection[:4].cpu().detach().numpy().astype('int')                \n                detections.append([x_min, y_min,x_max , y_max])\n\n        \n        if len(detections) == 0:\n            detections = np.empty((0, 5))\n        else:\n            detections = np.array(detections)\n\n            tracks = object_tracker.update(detections)\n            for track in tracks:\n                track_id = track[4]\n                setIDs.add(track_id)\n                bbox = track\n                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 0, 255), 2)\n                cv2.putText(frame, \"ID: \" + str(track_id), (int(bbox[0]), int(bbox[1] - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n            count = len(setIDs)   #count of the unique set IDs encountered throughout the video\n            cv2.putText(frame, f'Count: {int(count)}', (20, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n\n            # Write frame to output video\n        out.write(frame)\n    \n    # Release video writer\n    out.release()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:24.233868Z","iopub.execute_input":"2024-03-22T13:57:24.234113Z","iopub.status.idle":"2024-03-22T13:57:24.251186Z","shell.execute_reply.started":"2024-03-22T13:57:24.234092Z","shell.execute_reply":"2024-03-22T13:57:24.250215Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"video = cv2.VideoCapture('/kaggle/input/car-detection/Brigade_Road_1.mp4')\nTrackCarSort(model,video,2,object_tracker,'/kaggle/working/outputBrigade_YOLO+DeepSORT.mp4')","metadata":{"execution":{"iopub.status.busy":"2024-03-22T13:57:24.265928Z","iopub.execute_input":"2024-03-22T13:57:24.266267Z","iopub.status.idle":"2024-03-22T13:57:24.277579Z","shell.execute_reply.started":"2024-03-22T13:57:24.266233Z","shell.execute_reply":"2024-03-22T13:57:24.276877Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#Uncomment this to use DeepSORT ----------------------------------------------------------------------------------\n\n\n#car_id = 2 \n# confidence_threshold = 0.9 #0.8\n# object_type = ['car']\n\n# cap = cv2.VideoCapture('/kaggle/input/car-detection/Brigade_Road_1.mp4')\n\n# width = int(cap.get(3))\n# height = int(cap.get(4))\n\n# out = cv2.VideoWriter('outputBrigade_YOLO+DeepSORT.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 15, (width, height))\n\n# model.eval()\n\n# prev_count = 0\n# prev_max_id = 0\n\n# setIDs=set()\n# while cap.isOpened():\n    \n#     ret, frame = cap.read()\n#     start = time.perf_counter()\n#     if not ret:\n#         break\n    \n    \n#     with torch.no_grad(): \n#       pred = model(frame)\n        \n#     detections = [] \n#     for detection in pred.xyxy[0]:\n#         class_id = int(detection[5])\n#         confidence = detection[4]\n#         if confidence> confidence_threshold and class_id ==car_id:\n#             x_min,y_min,x_max,y_max = detection[:4].cpu().detach().numpy().astype('int')\n#             x,y,w,h = [x_min,y_min,int(x_max-x_min),int(y_max-y_min)]\n            \n#             cv2.rectangle(frame,(int(x_min), int(y_min)),(int(x_max), int(y_max)),(0,255,0),2)\n#             cv2.putText(frame, \"ID: \" + str(track_id), (int(x_min-5), int(y_min - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n            \n            \n#             detections.append(([x,y,w,h],confidence,'car'))\n            \n#     #Tracks for DeepSORT and Annotate \n#     tracks = object_tracker.update_tracks(detections, frame=frame)\n#     for track in tracks:\n#         if not track.is_confirmed():\n#             continue\n#         track_id = track.track_id\n#         setIDs.add(track_id)\n#         ltrb = track.to_ltrb()\n#         bbox = ltrb\n\n#     count=len(setIDs)\n#     cv2.putText(frame, f'Count: {int(count)}', (20,200), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n    \n#     out.write(frame)\n# out.release()\n\n\n# print(\"Count of cars = \"+str(count))","metadata":{"id":"FPyT4t2mf4Wr","execution":{"iopub.status.busy":"2024-03-22T13:57:24.278798Z","iopub.execute_input":"2024-03-22T13:57:24.279088Z","iopub.status.idle":"2024-03-22T13:57:36.275678Z","shell.execute_reply.started":"2024-03-22T13:57:24.279046Z","shell.execute_reply":"2024-03-22T13:57:36.274814Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"WARNING ⚠️ NMS time limit 0.550s exceeded\n","output_type":"stream"}]}]}