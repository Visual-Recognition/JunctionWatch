{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3173719,"sourceType":"datasetVersion","datasetId":952827},{"sourceId":7832883,"sourceType":"datasetVersion","datasetId":4590844},{"sourceId":15981,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":13316}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:56:43.849651Z","iopub.execute_input":"2024-03-13T12:56:43.850338Z","iopub.status.idle":"2024-03-13T12:56:56.860916Z","shell.execute_reply.started":"2024-03-13T12:56:43.850295Z","shell.execute_reply":"2024-03-13T12:56:56.859972Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch.nn import Module\nfrom torch.nn import Sequential\nfrom torch.nn import Conv2d\nfrom torch.nn import Linear\nfrom torch.nn import MaxPool2d\nfrom torch.nn import AvgPool2d\nfrom torch.nn import BatchNorm2d\nfrom torch.nn import ReLU\nfrom torch.nn import Sigmoid\nfrom torch.nn import Tanh\nfrom torch.nn import ELU\nfrom torch.nn import LogSoftmax\nfrom torch.nn import Dropout\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom torch import flatten\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms.autoaugment import AutoAugmentPolicy\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import ToTensor\nfrom torchsummary import summary\nimport cv2\nimport glob\nimport pickle\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-13T12:56:56.862621Z","iopub.execute_input":"2024-03-13T12:56:56.862936Z","iopub.status.idle":"2024-03-13T12:57:05.746559Z","shell.execute_reply.started":"2024-03-13T12:56:56.862910Z","shell.execute_reply":"2024-03-13T12:57:05.745570Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models import resnet50\n\nresNet=resnet50(pretrained=True).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:57:42.591946Z","iopub.execute_input":"2024-03-13T12:57:42.592300Z","iopub.status.idle":"2024-03-13T12:57:44.175655Z","shell.execute_reply.started":"2024-03-13T12:57:42.592273Z","shell.execute_reply":"2024-03-13T12:57:44.174625Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s] \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n\n  def __init__(self, model):\n    super(FeatureExtractor, self).__init__()\n    self.layer0=Sequential(\n        model.conv1,\n        model.bn1,\n        model.relu,\n        model.maxpool\n    )\n    self.layer1 = model.layer1\n    self.layer2=model.layer2\n    self.layer3=model.layer3\n    self.layer4=model.layer4\n    \n    self.pooling = nn.AdaptiveAvgPool2d(output_size=(2, 2))\n    \n    self.flatten = nn.Flatten()\n    \n  \n  def forward(self, x):\n    out=self.layer0(x)\n    out = self.layer1(out)\n    out=self.layer2(out)\n    out=self.layer3(out)\n    out=self.layer4(out)\n    out = self.pooling(out)\n    out = self.flatten(out)\n    return out \n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:57:38.098432Z","iopub.execute_input":"2024-03-13T12:57:38.098834Z","iopub.status.idle":"2024-03-13T12:57:38.106890Z","shell.execute_reply.started":"2024-03-13T12:57:38.098803Z","shell.execute_reply":"2024-03-13T12:57:38.105960Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((720, 720)),\n    transforms.ToTensor()\n])\ntrain_data_path = '/kaggle/input/fruit-and-vegetable-image-recognition/train'\n\ntrain_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\ntrain_loader.dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:44:43.733973Z","iopub.execute_input":"2024-03-13T12:44:43.734339Z","iopub.status.idle":"2024-03-13T12:44:44.754581Z","shell.execute_reply.started":"2024-03-13T12:44:43.734310Z","shell.execute_reply":"2024-03-13T12:44:44.753604Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset ImageFolder\n    Number of datapoints: 3115\n    Root location: /kaggle/input/fruit-and-vegetable-image-recognition/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(720, 720), interpolation=bilinear, max_size=None, antialias=warn)\n               ToTensor()\n           )"},"metadata":{}}]},{"cell_type":"code","source":"test_data_path = '/kaggle/input/fruit-and-vegetable-image-recognition/test'\ntest_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:44:47.029024Z","iopub.execute_input":"2024-03-13T12:44:47.029378Z","iopub.status.idle":"2024-03-13T12:44:47.279491Z","shell.execute_reply.started":"2024-03-13T12:44:47.029352Z","shell.execute_reply":"2024-03-13T12:44:47.278424Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class NewFeatureExtractor(FeatureExtractor):\n\n    def __init__(self, model, num_classes):\n        super(NewFeatureExtractor, self).__init__(model)\n        # Add a fully connected layer\n        self.fc = Sequential(\n            nn.Linear(in_features=2048*2*2, out_features=2048),\n            ReLU(),\n            Dropout(0.3),\n            nn.Linear(in_features=2048,out_features=512),\n            ReLU(),\n            Dropout(0.3),\n            nn.Linear(in_features=512,out_features=num_classes)\n        )\n\n    def forward(self, x):\n        # Call the forward method of the parent class\n        out = super(NewFeatureExtractor, self).forward(x)\n        # Pass the output through the fully connected layer\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:57:49.288042Z","iopub.execute_input":"2024-03-13T12:57:49.288774Z","iopub.status.idle":"2024-03-13T12:57:49.295745Z","shell.execute_reply.started":"2024-03-13T12:57:49.288743Z","shell.execute_reply":"2024-03-13T12:57:49.294762Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"newModel=NewFeatureExtractor(resNet,36).to(device)\nnum_epochs = 25\nlearning_rate=0.001\ncriterion = nn.CrossEntropyLoss()\n\ncheckpoint= torch.load('/kaggle/input/resnet/pytorch/updated/1/modelNew.pth')\nnewModel.load_state_dict(checkpoint)\n\noptimizer = optim.Adam(newModel.parameters(), lr=learning_rate)\nnewModel.eval()\nsummary(newModel,(3,720,720))","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:43:41.273682Z","iopub.execute_input":"2024-03-13T12:43:41.274366Z","iopub.status.idle":"2024-03-13T12:43:43.218016Z","shell.execute_reply.started":"2024-03-13T12:43:41.274336Z","shell.execute_reply":"2024-03-13T12:43:43.217023Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 360, 360]           9,408\n       BatchNorm2d-2         [-1, 64, 360, 360]             128\n              ReLU-3         [-1, 64, 360, 360]               0\n         MaxPool2d-4         [-1, 64, 180, 180]               0\n            Conv2d-5         [-1, 64, 180, 180]           4,096\n       BatchNorm2d-6         [-1, 64, 180, 180]             128\n              ReLU-7         [-1, 64, 180, 180]               0\n            Conv2d-8         [-1, 64, 180, 180]          36,864\n       BatchNorm2d-9         [-1, 64, 180, 180]             128\n             ReLU-10         [-1, 64, 180, 180]               0\n           Conv2d-11        [-1, 256, 180, 180]          16,384\n      BatchNorm2d-12        [-1, 256, 180, 180]             512\n           Conv2d-13        [-1, 256, 180, 180]          16,384\n      BatchNorm2d-14        [-1, 256, 180, 180]             512\n             ReLU-15        [-1, 256, 180, 180]               0\n       Bottleneck-16        [-1, 256, 180, 180]               0\n           Conv2d-17         [-1, 64, 180, 180]          16,384\n      BatchNorm2d-18         [-1, 64, 180, 180]             128\n             ReLU-19         [-1, 64, 180, 180]               0\n           Conv2d-20         [-1, 64, 180, 180]          36,864\n      BatchNorm2d-21         [-1, 64, 180, 180]             128\n             ReLU-22         [-1, 64, 180, 180]               0\n           Conv2d-23        [-1, 256, 180, 180]          16,384\n      BatchNorm2d-24        [-1, 256, 180, 180]             512\n             ReLU-25        [-1, 256, 180, 180]               0\n       Bottleneck-26        [-1, 256, 180, 180]               0\n           Conv2d-27         [-1, 64, 180, 180]          16,384\n      BatchNorm2d-28         [-1, 64, 180, 180]             128\n             ReLU-29         [-1, 64, 180, 180]               0\n           Conv2d-30         [-1, 64, 180, 180]          36,864\n      BatchNorm2d-31         [-1, 64, 180, 180]             128\n             ReLU-32         [-1, 64, 180, 180]               0\n           Conv2d-33        [-1, 256, 180, 180]          16,384\n      BatchNorm2d-34        [-1, 256, 180, 180]             512\n             ReLU-35        [-1, 256, 180, 180]               0\n       Bottleneck-36        [-1, 256, 180, 180]               0\n           Conv2d-37        [-1, 128, 180, 180]          32,768\n      BatchNorm2d-38        [-1, 128, 180, 180]             256\n             ReLU-39        [-1, 128, 180, 180]               0\n           Conv2d-40          [-1, 128, 90, 90]         147,456\n      BatchNorm2d-41          [-1, 128, 90, 90]             256\n             ReLU-42          [-1, 128, 90, 90]               0\n           Conv2d-43          [-1, 512, 90, 90]          65,536\n      BatchNorm2d-44          [-1, 512, 90, 90]           1,024\n           Conv2d-45          [-1, 512, 90, 90]         131,072\n      BatchNorm2d-46          [-1, 512, 90, 90]           1,024\n             ReLU-47          [-1, 512, 90, 90]               0\n       Bottleneck-48          [-1, 512, 90, 90]               0\n           Conv2d-49          [-1, 128, 90, 90]          65,536\n      BatchNorm2d-50          [-1, 128, 90, 90]             256\n             ReLU-51          [-1, 128, 90, 90]               0\n           Conv2d-52          [-1, 128, 90, 90]         147,456\n      BatchNorm2d-53          [-1, 128, 90, 90]             256\n             ReLU-54          [-1, 128, 90, 90]               0\n           Conv2d-55          [-1, 512, 90, 90]          65,536\n      BatchNorm2d-56          [-1, 512, 90, 90]           1,024\n             ReLU-57          [-1, 512, 90, 90]               0\n       Bottleneck-58          [-1, 512, 90, 90]               0\n           Conv2d-59          [-1, 128, 90, 90]          65,536\n      BatchNorm2d-60          [-1, 128, 90, 90]             256\n             ReLU-61          [-1, 128, 90, 90]               0\n           Conv2d-62          [-1, 128, 90, 90]         147,456\n      BatchNorm2d-63          [-1, 128, 90, 90]             256\n             ReLU-64          [-1, 128, 90, 90]               0\n           Conv2d-65          [-1, 512, 90, 90]          65,536\n      BatchNorm2d-66          [-1, 512, 90, 90]           1,024\n             ReLU-67          [-1, 512, 90, 90]               0\n       Bottleneck-68          [-1, 512, 90, 90]               0\n           Conv2d-69          [-1, 128, 90, 90]          65,536\n      BatchNorm2d-70          [-1, 128, 90, 90]             256\n             ReLU-71          [-1, 128, 90, 90]               0\n           Conv2d-72          [-1, 128, 90, 90]         147,456\n      BatchNorm2d-73          [-1, 128, 90, 90]             256\n             ReLU-74          [-1, 128, 90, 90]               0\n           Conv2d-75          [-1, 512, 90, 90]          65,536\n      BatchNorm2d-76          [-1, 512, 90, 90]           1,024\n             ReLU-77          [-1, 512, 90, 90]               0\n       Bottleneck-78          [-1, 512, 90, 90]               0\n           Conv2d-79          [-1, 256, 90, 90]         131,072\n      BatchNorm2d-80          [-1, 256, 90, 90]             512\n             ReLU-81          [-1, 256, 90, 90]               0\n           Conv2d-82          [-1, 256, 45, 45]         589,824\n      BatchNorm2d-83          [-1, 256, 45, 45]             512\n             ReLU-84          [-1, 256, 45, 45]               0\n           Conv2d-85         [-1, 1024, 45, 45]         262,144\n      BatchNorm2d-86         [-1, 1024, 45, 45]           2,048\n           Conv2d-87         [-1, 1024, 45, 45]         524,288\n      BatchNorm2d-88         [-1, 1024, 45, 45]           2,048\n             ReLU-89         [-1, 1024, 45, 45]               0\n       Bottleneck-90         [-1, 1024, 45, 45]               0\n           Conv2d-91          [-1, 256, 45, 45]         262,144\n      BatchNorm2d-92          [-1, 256, 45, 45]             512\n             ReLU-93          [-1, 256, 45, 45]               0\n           Conv2d-94          [-1, 256, 45, 45]         589,824\n      BatchNorm2d-95          [-1, 256, 45, 45]             512\n             ReLU-96          [-1, 256, 45, 45]               0\n           Conv2d-97         [-1, 1024, 45, 45]         262,144\n      BatchNorm2d-98         [-1, 1024, 45, 45]           2,048\n             ReLU-99         [-1, 1024, 45, 45]               0\n      Bottleneck-100         [-1, 1024, 45, 45]               0\n          Conv2d-101          [-1, 256, 45, 45]         262,144\n     BatchNorm2d-102          [-1, 256, 45, 45]             512\n            ReLU-103          [-1, 256, 45, 45]               0\n          Conv2d-104          [-1, 256, 45, 45]         589,824\n     BatchNorm2d-105          [-1, 256, 45, 45]             512\n            ReLU-106          [-1, 256, 45, 45]               0\n          Conv2d-107         [-1, 1024, 45, 45]         262,144\n     BatchNorm2d-108         [-1, 1024, 45, 45]           2,048\n            ReLU-109         [-1, 1024, 45, 45]               0\n      Bottleneck-110         [-1, 1024, 45, 45]               0\n          Conv2d-111          [-1, 256, 45, 45]         262,144\n     BatchNorm2d-112          [-1, 256, 45, 45]             512\n            ReLU-113          [-1, 256, 45, 45]               0\n          Conv2d-114          [-1, 256, 45, 45]         589,824\n     BatchNorm2d-115          [-1, 256, 45, 45]             512\n            ReLU-116          [-1, 256, 45, 45]               0\n          Conv2d-117         [-1, 1024, 45, 45]         262,144\n     BatchNorm2d-118         [-1, 1024, 45, 45]           2,048\n            ReLU-119         [-1, 1024, 45, 45]               0\n      Bottleneck-120         [-1, 1024, 45, 45]               0\n          Conv2d-121          [-1, 256, 45, 45]         262,144\n     BatchNorm2d-122          [-1, 256, 45, 45]             512\n            ReLU-123          [-1, 256, 45, 45]               0\n          Conv2d-124          [-1, 256, 45, 45]         589,824\n     BatchNorm2d-125          [-1, 256, 45, 45]             512\n            ReLU-126          [-1, 256, 45, 45]               0\n          Conv2d-127         [-1, 1024, 45, 45]         262,144\n     BatchNorm2d-128         [-1, 1024, 45, 45]           2,048\n            ReLU-129         [-1, 1024, 45, 45]               0\n      Bottleneck-130         [-1, 1024, 45, 45]               0\n          Conv2d-131          [-1, 256, 45, 45]         262,144\n     BatchNorm2d-132          [-1, 256, 45, 45]             512\n            ReLU-133          [-1, 256, 45, 45]               0\n          Conv2d-134          [-1, 256, 45, 45]         589,824\n     BatchNorm2d-135          [-1, 256, 45, 45]             512\n            ReLU-136          [-1, 256, 45, 45]               0\n          Conv2d-137         [-1, 1024, 45, 45]         262,144\n     BatchNorm2d-138         [-1, 1024, 45, 45]           2,048\n            ReLU-139         [-1, 1024, 45, 45]               0\n      Bottleneck-140         [-1, 1024, 45, 45]               0\n          Conv2d-141          [-1, 512, 45, 45]         524,288\n     BatchNorm2d-142          [-1, 512, 45, 45]           1,024\n            ReLU-143          [-1, 512, 45, 45]               0\n          Conv2d-144          [-1, 512, 23, 23]       2,359,296\n     BatchNorm2d-145          [-1, 512, 23, 23]           1,024\n            ReLU-146          [-1, 512, 23, 23]               0\n          Conv2d-147         [-1, 2048, 23, 23]       1,048,576\n     BatchNorm2d-148         [-1, 2048, 23, 23]           4,096\n          Conv2d-149         [-1, 2048, 23, 23]       2,097,152\n     BatchNorm2d-150         [-1, 2048, 23, 23]           4,096\n            ReLU-151         [-1, 2048, 23, 23]               0\n      Bottleneck-152         [-1, 2048, 23, 23]               0\n          Conv2d-153          [-1, 512, 23, 23]       1,048,576\n     BatchNorm2d-154          [-1, 512, 23, 23]           1,024\n            ReLU-155          [-1, 512, 23, 23]               0\n          Conv2d-156          [-1, 512, 23, 23]       2,359,296\n     BatchNorm2d-157          [-1, 512, 23, 23]           1,024\n            ReLU-158          [-1, 512, 23, 23]               0\n          Conv2d-159         [-1, 2048, 23, 23]       1,048,576\n     BatchNorm2d-160         [-1, 2048, 23, 23]           4,096\n            ReLU-161         [-1, 2048, 23, 23]               0\n      Bottleneck-162         [-1, 2048, 23, 23]               0\n          Conv2d-163          [-1, 512, 23, 23]       1,048,576\n     BatchNorm2d-164          [-1, 512, 23, 23]           1,024\n            ReLU-165          [-1, 512, 23, 23]               0\n          Conv2d-166          [-1, 512, 23, 23]       2,359,296\n     BatchNorm2d-167          [-1, 512, 23, 23]           1,024\n            ReLU-168          [-1, 512, 23, 23]               0\n          Conv2d-169         [-1, 2048, 23, 23]       1,048,576\n     BatchNorm2d-170         [-1, 2048, 23, 23]           4,096\n            ReLU-171         [-1, 2048, 23, 23]               0\n      Bottleneck-172         [-1, 2048, 23, 23]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 2, 2]               0\n         Flatten-174                 [-1, 8192]               0\n          Linear-175                 [-1, 2048]      16,779,264\n            ReLU-176                 [-1, 2048]               0\n         Dropout-177                 [-1, 2048]               0\n          Linear-178                  [-1, 512]       1,049,088\n            ReLU-179                  [-1, 512]               0\n         Dropout-180                  [-1, 512]               0\n          Linear-181                   [-1, 36]          18,468\n================================================================\nTotal params: 41,354,852\nTrainable params: 41,354,852\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 5.93\nForward/backward pass size (MB): 2966.87\nParams size (MB): 157.76\nEstimated Total Size (MB): 3130.56\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"epoch_losses = []\n\nfor epoch in range(num_epochs):\n    newModel.train()\n    running_loss = 0.0\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n        for images, labels in tepoch:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = newModel(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n            tepoch.set_postfix(loss=running_loss / ((tepoch.n + 1) * train_loader.batch_size))\n\n    # Print statistics\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_losses.append(epoch_loss)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T10:24:08.484835Z","iopub.execute_input":"2024-03-13T10:24:08.485507Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/195 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1869e7f7a3ae477180eafcf4bddf7df5"}},"metadata":{}}]},{"cell_type":"code","source":"torch.save(newModel.state_dict(), 'modelNew.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def validate(model, dataloader, sanity_check=False):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    truths = []\n    loss_fn = torch.nn.CrossEntropyLoss()\n\n    device = next(model.parameters()).device  # Get the device of the model's parameters\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            x, y = batch[0].to(device), batch[1].to(device)  # Move input data to the same device as model\n            \n            output = model(x)\n            output = nn.Softmax(dim=-1)(output)\n            loss = loss_fn(output, y)\n            total_loss += loss.detach().cpu().item() / len(dataloader)\n            \n            preds = torch.argmax(output, dim=-1)\n            predictions.extend(preds.cpu())\n            truths.extend(y.cpu())\n        \n    acc = accuracy_score(y_true=truths, y_pred=predictions)\n    f1 = f1_score(y_true=truths, y_pred=predictions, average='macro')\n    \n    return total_loss, acc, f1, predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T12:57:59.599838Z","iopub.execute_input":"2024-03-13T12:57:59.600531Z","iopub.status.idle":"2024-03-13T12:57:59.608736Z","shell.execute_reply.started":"2024-03-13T12:57:59.600500Z","shell.execute_reply":"2024-03-13T12:57:59.607845Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"scores = validate(newModel,test_loader)\nprint(f\"| Test Loss: {scores[0]: 7.3f}  | Test acc: {scores[1]: 1.5f}  | Test f1: {scores[2]: 1.5f}  |\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T10:27:17.791655Z","iopub.execute_input":"2024-03-13T10:27:17.792041Z","iopub.status.idle":"2024-03-13T10:27:50.180024Z","shell.execute_reply.started":"2024-03-13T10:27:17.792011Z","shell.execute_reply":"2024-03-13T10:27:50.178990Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375f13f1baef467bb8a85f3838acc43f"}},"metadata":{}},{"name":"stdout","text":"| Test Loss:   2.695  | Test acc:  0.96379  | Test f1:  0.96311  |\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bike And Horse Dataset","metadata":{}},{"cell_type":"code","source":"feature_extractor = FeatureExtractor(resNet)\nfeature_extractor = feature_extractor.to(device)\nsummary(feature_extractor,(3,280,470))","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 140, 235]           9,408\n       BatchNorm2d-2         [-1, 64, 140, 235]             128\n              ReLU-3         [-1, 64, 140, 235]               0\n         MaxPool2d-4          [-1, 64, 70, 118]               0\n            Conv2d-5          [-1, 64, 70, 118]           4,096\n       BatchNorm2d-6          [-1, 64, 70, 118]             128\n              ReLU-7          [-1, 64, 70, 118]               0\n            Conv2d-8          [-1, 64, 70, 118]          36,864\n       BatchNorm2d-9          [-1, 64, 70, 118]             128\n             ReLU-10          [-1, 64, 70, 118]               0\n           Conv2d-11         [-1, 256, 70, 118]          16,384\n      BatchNorm2d-12         [-1, 256, 70, 118]             512\n           Conv2d-13         [-1, 256, 70, 118]          16,384\n      BatchNorm2d-14         [-1, 256, 70, 118]             512\n             ReLU-15         [-1, 256, 70, 118]               0\n       Bottleneck-16         [-1, 256, 70, 118]               0\n           Conv2d-17          [-1, 64, 70, 118]          16,384\n      BatchNorm2d-18          [-1, 64, 70, 118]             128\n             ReLU-19          [-1, 64, 70, 118]               0\n           Conv2d-20          [-1, 64, 70, 118]          36,864\n      BatchNorm2d-21          [-1, 64, 70, 118]             128\n             ReLU-22          [-1, 64, 70, 118]               0\n           Conv2d-23         [-1, 256, 70, 118]          16,384\n      BatchNorm2d-24         [-1, 256, 70, 118]             512\n             ReLU-25         [-1, 256, 70, 118]               0\n       Bottleneck-26         [-1, 256, 70, 118]               0\n           Conv2d-27          [-1, 64, 70, 118]          16,384\n      BatchNorm2d-28          [-1, 64, 70, 118]             128\n             ReLU-29          [-1, 64, 70, 118]               0\n           Conv2d-30          [-1, 64, 70, 118]          36,864\n      BatchNorm2d-31          [-1, 64, 70, 118]             128\n             ReLU-32          [-1, 64, 70, 118]               0\n           Conv2d-33         [-1, 256, 70, 118]          16,384\n      BatchNorm2d-34         [-1, 256, 70, 118]             512\n             ReLU-35         [-1, 256, 70, 118]               0\n       Bottleneck-36         [-1, 256, 70, 118]               0\n           Conv2d-37         [-1, 128, 70, 118]          32,768\n      BatchNorm2d-38         [-1, 128, 70, 118]             256\n             ReLU-39         [-1, 128, 70, 118]               0\n           Conv2d-40          [-1, 128, 35, 59]         147,456\n      BatchNorm2d-41          [-1, 128, 35, 59]             256\n             ReLU-42          [-1, 128, 35, 59]               0\n           Conv2d-43          [-1, 512, 35, 59]          65,536\n      BatchNorm2d-44          [-1, 512, 35, 59]           1,024\n           Conv2d-45          [-1, 512, 35, 59]         131,072\n      BatchNorm2d-46          [-1, 512, 35, 59]           1,024\n             ReLU-47          [-1, 512, 35, 59]               0\n       Bottleneck-48          [-1, 512, 35, 59]               0\n           Conv2d-49          [-1, 128, 35, 59]          65,536\n      BatchNorm2d-50          [-1, 128, 35, 59]             256\n             ReLU-51          [-1, 128, 35, 59]               0\n           Conv2d-52          [-1, 128, 35, 59]         147,456\n      BatchNorm2d-53          [-1, 128, 35, 59]             256\n             ReLU-54          [-1, 128, 35, 59]               0\n           Conv2d-55          [-1, 512, 35, 59]          65,536\n      BatchNorm2d-56          [-1, 512, 35, 59]           1,024\n             ReLU-57          [-1, 512, 35, 59]               0\n       Bottleneck-58          [-1, 512, 35, 59]               0\n           Conv2d-59          [-1, 128, 35, 59]          65,536\n      BatchNorm2d-60          [-1, 128, 35, 59]             256\n             ReLU-61          [-1, 128, 35, 59]               0\n           Conv2d-62          [-1, 128, 35, 59]         147,456\n      BatchNorm2d-63          [-1, 128, 35, 59]             256\n             ReLU-64          [-1, 128, 35, 59]               0\n           Conv2d-65          [-1, 512, 35, 59]          65,536\n      BatchNorm2d-66          [-1, 512, 35, 59]           1,024\n             ReLU-67          [-1, 512, 35, 59]               0\n       Bottleneck-68          [-1, 512, 35, 59]               0\n           Conv2d-69          [-1, 128, 35, 59]          65,536\n      BatchNorm2d-70          [-1, 128, 35, 59]             256\n             ReLU-71          [-1, 128, 35, 59]               0\n           Conv2d-72          [-1, 128, 35, 59]         147,456\n      BatchNorm2d-73          [-1, 128, 35, 59]             256\n             ReLU-74          [-1, 128, 35, 59]               0\n           Conv2d-75          [-1, 512, 35, 59]          65,536\n      BatchNorm2d-76          [-1, 512, 35, 59]           1,024\n             ReLU-77          [-1, 512, 35, 59]               0\n       Bottleneck-78          [-1, 512, 35, 59]               0\n           Conv2d-79          [-1, 256, 35, 59]         131,072\n      BatchNorm2d-80          [-1, 256, 35, 59]             512\n             ReLU-81          [-1, 256, 35, 59]               0\n           Conv2d-82          [-1, 256, 18, 30]         589,824\n      BatchNorm2d-83          [-1, 256, 18, 30]             512\n             ReLU-84          [-1, 256, 18, 30]               0\n           Conv2d-85         [-1, 1024, 18, 30]         262,144\n      BatchNorm2d-86         [-1, 1024, 18, 30]           2,048\n           Conv2d-87         [-1, 1024, 18, 30]         524,288\n      BatchNorm2d-88         [-1, 1024, 18, 30]           2,048\n             ReLU-89         [-1, 1024, 18, 30]               0\n       Bottleneck-90         [-1, 1024, 18, 30]               0\n           Conv2d-91          [-1, 256, 18, 30]         262,144\n      BatchNorm2d-92          [-1, 256, 18, 30]             512\n             ReLU-93          [-1, 256, 18, 30]               0\n           Conv2d-94          [-1, 256, 18, 30]         589,824\n      BatchNorm2d-95          [-1, 256, 18, 30]             512\n             ReLU-96          [-1, 256, 18, 30]               0\n           Conv2d-97         [-1, 1024, 18, 30]         262,144\n      BatchNorm2d-98         [-1, 1024, 18, 30]           2,048\n             ReLU-99         [-1, 1024, 18, 30]               0\n      Bottleneck-100         [-1, 1024, 18, 30]               0\n          Conv2d-101          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-102          [-1, 256, 18, 30]             512\n            ReLU-103          [-1, 256, 18, 30]               0\n          Conv2d-104          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-105          [-1, 256, 18, 30]             512\n            ReLU-106          [-1, 256, 18, 30]               0\n          Conv2d-107         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-108         [-1, 1024, 18, 30]           2,048\n            ReLU-109         [-1, 1024, 18, 30]               0\n      Bottleneck-110         [-1, 1024, 18, 30]               0\n          Conv2d-111          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-112          [-1, 256, 18, 30]             512\n            ReLU-113          [-1, 256, 18, 30]               0\n          Conv2d-114          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-115          [-1, 256, 18, 30]             512\n            ReLU-116          [-1, 256, 18, 30]               0\n          Conv2d-117         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-118         [-1, 1024, 18, 30]           2,048\n            ReLU-119         [-1, 1024, 18, 30]               0\n      Bottleneck-120         [-1, 1024, 18, 30]               0\n          Conv2d-121          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-122          [-1, 256, 18, 30]             512\n            ReLU-123          [-1, 256, 18, 30]               0\n          Conv2d-124          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-125          [-1, 256, 18, 30]             512\n            ReLU-126          [-1, 256, 18, 30]               0\n          Conv2d-127         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-128         [-1, 1024, 18, 30]           2,048\n            ReLU-129         [-1, 1024, 18, 30]               0\n      Bottleneck-130         [-1, 1024, 18, 30]               0\n          Conv2d-131          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-132          [-1, 256, 18, 30]             512\n            ReLU-133          [-1, 256, 18, 30]               0\n          Conv2d-134          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-135          [-1, 256, 18, 30]             512\n            ReLU-136          [-1, 256, 18, 30]               0\n          Conv2d-137         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-138         [-1, 1024, 18, 30]           2,048\n            ReLU-139         [-1, 1024, 18, 30]               0\n      Bottleneck-140         [-1, 1024, 18, 30]               0\n          Conv2d-141          [-1, 512, 18, 30]         524,288\n     BatchNorm2d-142          [-1, 512, 18, 30]           1,024\n            ReLU-143          [-1, 512, 18, 30]               0\n          Conv2d-144           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-145           [-1, 512, 9, 15]           1,024\n            ReLU-146           [-1, 512, 9, 15]               0\n          Conv2d-147          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-148          [-1, 2048, 9, 15]           4,096\n          Conv2d-149          [-1, 2048, 9, 15]       2,097,152\n     BatchNorm2d-150          [-1, 2048, 9, 15]           4,096\n            ReLU-151          [-1, 2048, 9, 15]               0\n      Bottleneck-152          [-1, 2048, 9, 15]               0\n          Conv2d-153           [-1, 512, 9, 15]       1,048,576\n     BatchNorm2d-154           [-1, 512, 9, 15]           1,024\n            ReLU-155           [-1, 512, 9, 15]               0\n          Conv2d-156           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-157           [-1, 512, 9, 15]           1,024\n            ReLU-158           [-1, 512, 9, 15]               0\n          Conv2d-159          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-160          [-1, 2048, 9, 15]           4,096\n            ReLU-161          [-1, 2048, 9, 15]               0\n      Bottleneck-162          [-1, 2048, 9, 15]               0\n          Conv2d-163           [-1, 512, 9, 15]       1,048,576\n     BatchNorm2d-164           [-1, 512, 9, 15]           1,024\n            ReLU-165           [-1, 512, 9, 15]               0\n          Conv2d-166           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-167           [-1, 512, 9, 15]           1,024\n            ReLU-168           [-1, 512, 9, 15]               0\n          Conv2d-169          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-170          [-1, 2048, 9, 15]           4,096\n            ReLU-171          [-1, 2048, 9, 15]               0\n      Bottleneck-172          [-1, 2048, 9, 15]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 2, 2]               0\n         Flatten-174                 [-1, 8192]               0\n================================================================\nTotal params: 23,508,032\nTrainable params: 23,508,032\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.51\nForward/backward pass size (MB): 762.91\nParams size (MB): 89.68\nEstimated Total Size (MB): 854.10\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"bikehorsemodel=NewFeatureExtractor(resNet,2).to(device)\nsummary(bikehorsemodel,(3,280,480))","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:03:13.505230Z","iopub.execute_input":"2024-03-13T13:03:13.505592Z","iopub.status.idle":"2024-03-13T13:03:13.739083Z","shell.execute_reply.started":"2024-03-13T13:03:13.505563Z","shell.execute_reply":"2024-03-13T13:03:13.738224Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 140, 240]           9,408\n       BatchNorm2d-2         [-1, 64, 140, 240]             128\n              ReLU-3         [-1, 64, 140, 240]               0\n         MaxPool2d-4          [-1, 64, 70, 120]               0\n            Conv2d-5          [-1, 64, 70, 120]           4,096\n       BatchNorm2d-6          [-1, 64, 70, 120]             128\n              ReLU-7          [-1, 64, 70, 120]               0\n            Conv2d-8          [-1, 64, 70, 120]          36,864\n       BatchNorm2d-9          [-1, 64, 70, 120]             128\n             ReLU-10          [-1, 64, 70, 120]               0\n           Conv2d-11         [-1, 256, 70, 120]          16,384\n      BatchNorm2d-12         [-1, 256, 70, 120]             512\n           Conv2d-13         [-1, 256, 70, 120]          16,384\n      BatchNorm2d-14         [-1, 256, 70, 120]             512\n             ReLU-15         [-1, 256, 70, 120]               0\n       Bottleneck-16         [-1, 256, 70, 120]               0\n           Conv2d-17          [-1, 64, 70, 120]          16,384\n      BatchNorm2d-18          [-1, 64, 70, 120]             128\n             ReLU-19          [-1, 64, 70, 120]               0\n           Conv2d-20          [-1, 64, 70, 120]          36,864\n      BatchNorm2d-21          [-1, 64, 70, 120]             128\n             ReLU-22          [-1, 64, 70, 120]               0\n           Conv2d-23         [-1, 256, 70, 120]          16,384\n      BatchNorm2d-24         [-1, 256, 70, 120]             512\n             ReLU-25         [-1, 256, 70, 120]               0\n       Bottleneck-26         [-1, 256, 70, 120]               0\n           Conv2d-27          [-1, 64, 70, 120]          16,384\n      BatchNorm2d-28          [-1, 64, 70, 120]             128\n             ReLU-29          [-1, 64, 70, 120]               0\n           Conv2d-30          [-1, 64, 70, 120]          36,864\n      BatchNorm2d-31          [-1, 64, 70, 120]             128\n             ReLU-32          [-1, 64, 70, 120]               0\n           Conv2d-33         [-1, 256, 70, 120]          16,384\n      BatchNorm2d-34         [-1, 256, 70, 120]             512\n             ReLU-35         [-1, 256, 70, 120]               0\n       Bottleneck-36         [-1, 256, 70, 120]               0\n           Conv2d-37         [-1, 128, 70, 120]          32,768\n      BatchNorm2d-38         [-1, 128, 70, 120]             256\n             ReLU-39         [-1, 128, 70, 120]               0\n           Conv2d-40          [-1, 128, 35, 60]         147,456\n      BatchNorm2d-41          [-1, 128, 35, 60]             256\n             ReLU-42          [-1, 128, 35, 60]               0\n           Conv2d-43          [-1, 512, 35, 60]          65,536\n      BatchNorm2d-44          [-1, 512, 35, 60]           1,024\n           Conv2d-45          [-1, 512, 35, 60]         131,072\n      BatchNorm2d-46          [-1, 512, 35, 60]           1,024\n             ReLU-47          [-1, 512, 35, 60]               0\n       Bottleneck-48          [-1, 512, 35, 60]               0\n           Conv2d-49          [-1, 128, 35, 60]          65,536\n      BatchNorm2d-50          [-1, 128, 35, 60]             256\n             ReLU-51          [-1, 128, 35, 60]               0\n           Conv2d-52          [-1, 128, 35, 60]         147,456\n      BatchNorm2d-53          [-1, 128, 35, 60]             256\n             ReLU-54          [-1, 128, 35, 60]               0\n           Conv2d-55          [-1, 512, 35, 60]          65,536\n      BatchNorm2d-56          [-1, 512, 35, 60]           1,024\n             ReLU-57          [-1, 512, 35, 60]               0\n       Bottleneck-58          [-1, 512, 35, 60]               0\n           Conv2d-59          [-1, 128, 35, 60]          65,536\n      BatchNorm2d-60          [-1, 128, 35, 60]             256\n             ReLU-61          [-1, 128, 35, 60]               0\n           Conv2d-62          [-1, 128, 35, 60]         147,456\n      BatchNorm2d-63          [-1, 128, 35, 60]             256\n             ReLU-64          [-1, 128, 35, 60]               0\n           Conv2d-65          [-1, 512, 35, 60]          65,536\n      BatchNorm2d-66          [-1, 512, 35, 60]           1,024\n             ReLU-67          [-1, 512, 35, 60]               0\n       Bottleneck-68          [-1, 512, 35, 60]               0\n           Conv2d-69          [-1, 128, 35, 60]          65,536\n      BatchNorm2d-70          [-1, 128, 35, 60]             256\n             ReLU-71          [-1, 128, 35, 60]               0\n           Conv2d-72          [-1, 128, 35, 60]         147,456\n      BatchNorm2d-73          [-1, 128, 35, 60]             256\n             ReLU-74          [-1, 128, 35, 60]               0\n           Conv2d-75          [-1, 512, 35, 60]          65,536\n      BatchNorm2d-76          [-1, 512, 35, 60]           1,024\n             ReLU-77          [-1, 512, 35, 60]               0\n       Bottleneck-78          [-1, 512, 35, 60]               0\n           Conv2d-79          [-1, 256, 35, 60]         131,072\n      BatchNorm2d-80          [-1, 256, 35, 60]             512\n             ReLU-81          [-1, 256, 35, 60]               0\n           Conv2d-82          [-1, 256, 18, 30]         589,824\n      BatchNorm2d-83          [-1, 256, 18, 30]             512\n             ReLU-84          [-1, 256, 18, 30]               0\n           Conv2d-85         [-1, 1024, 18, 30]         262,144\n      BatchNorm2d-86         [-1, 1024, 18, 30]           2,048\n           Conv2d-87         [-1, 1024, 18, 30]         524,288\n      BatchNorm2d-88         [-1, 1024, 18, 30]           2,048\n             ReLU-89         [-1, 1024, 18, 30]               0\n       Bottleneck-90         [-1, 1024, 18, 30]               0\n           Conv2d-91          [-1, 256, 18, 30]         262,144\n      BatchNorm2d-92          [-1, 256, 18, 30]             512\n             ReLU-93          [-1, 256, 18, 30]               0\n           Conv2d-94          [-1, 256, 18, 30]         589,824\n      BatchNorm2d-95          [-1, 256, 18, 30]             512\n             ReLU-96          [-1, 256, 18, 30]               0\n           Conv2d-97         [-1, 1024, 18, 30]         262,144\n      BatchNorm2d-98         [-1, 1024, 18, 30]           2,048\n             ReLU-99         [-1, 1024, 18, 30]               0\n      Bottleneck-100         [-1, 1024, 18, 30]               0\n          Conv2d-101          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-102          [-1, 256, 18, 30]             512\n            ReLU-103          [-1, 256, 18, 30]               0\n          Conv2d-104          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-105          [-1, 256, 18, 30]             512\n            ReLU-106          [-1, 256, 18, 30]               0\n          Conv2d-107         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-108         [-1, 1024, 18, 30]           2,048\n            ReLU-109         [-1, 1024, 18, 30]               0\n      Bottleneck-110         [-1, 1024, 18, 30]               0\n          Conv2d-111          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-112          [-1, 256, 18, 30]             512\n            ReLU-113          [-1, 256, 18, 30]               0\n          Conv2d-114          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-115          [-1, 256, 18, 30]             512\n            ReLU-116          [-1, 256, 18, 30]               0\n          Conv2d-117         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-118         [-1, 1024, 18, 30]           2,048\n            ReLU-119         [-1, 1024, 18, 30]               0\n      Bottleneck-120         [-1, 1024, 18, 30]               0\n          Conv2d-121          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-122          [-1, 256, 18, 30]             512\n            ReLU-123          [-1, 256, 18, 30]               0\n          Conv2d-124          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-125          [-1, 256, 18, 30]             512\n            ReLU-126          [-1, 256, 18, 30]               0\n          Conv2d-127         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-128         [-1, 1024, 18, 30]           2,048\n            ReLU-129         [-1, 1024, 18, 30]               0\n      Bottleneck-130         [-1, 1024, 18, 30]               0\n          Conv2d-131          [-1, 256, 18, 30]         262,144\n     BatchNorm2d-132          [-1, 256, 18, 30]             512\n            ReLU-133          [-1, 256, 18, 30]               0\n          Conv2d-134          [-1, 256, 18, 30]         589,824\n     BatchNorm2d-135          [-1, 256, 18, 30]             512\n            ReLU-136          [-1, 256, 18, 30]               0\n          Conv2d-137         [-1, 1024, 18, 30]         262,144\n     BatchNorm2d-138         [-1, 1024, 18, 30]           2,048\n            ReLU-139         [-1, 1024, 18, 30]               0\n      Bottleneck-140         [-1, 1024, 18, 30]               0\n          Conv2d-141          [-1, 512, 18, 30]         524,288\n     BatchNorm2d-142          [-1, 512, 18, 30]           1,024\n            ReLU-143          [-1, 512, 18, 30]               0\n          Conv2d-144           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-145           [-1, 512, 9, 15]           1,024\n            ReLU-146           [-1, 512, 9, 15]               0\n          Conv2d-147          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-148          [-1, 2048, 9, 15]           4,096\n          Conv2d-149          [-1, 2048, 9, 15]       2,097,152\n     BatchNorm2d-150          [-1, 2048, 9, 15]           4,096\n            ReLU-151          [-1, 2048, 9, 15]               0\n      Bottleneck-152          [-1, 2048, 9, 15]               0\n          Conv2d-153           [-1, 512, 9, 15]       1,048,576\n     BatchNorm2d-154           [-1, 512, 9, 15]           1,024\n            ReLU-155           [-1, 512, 9, 15]               0\n          Conv2d-156           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-157           [-1, 512, 9, 15]           1,024\n            ReLU-158           [-1, 512, 9, 15]               0\n          Conv2d-159          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-160          [-1, 2048, 9, 15]           4,096\n            ReLU-161          [-1, 2048, 9, 15]               0\n      Bottleneck-162          [-1, 2048, 9, 15]               0\n          Conv2d-163           [-1, 512, 9, 15]       1,048,576\n     BatchNorm2d-164           [-1, 512, 9, 15]           1,024\n            ReLU-165           [-1, 512, 9, 15]               0\n          Conv2d-166           [-1, 512, 9, 15]       2,359,296\n     BatchNorm2d-167           [-1, 512, 9, 15]           1,024\n            ReLU-168           [-1, 512, 9, 15]               0\n          Conv2d-169          [-1, 2048, 9, 15]       1,048,576\n     BatchNorm2d-170          [-1, 2048, 9, 15]           4,096\n            ReLU-171          [-1, 2048, 9, 15]               0\n      Bottleneck-172          [-1, 2048, 9, 15]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 2, 2]               0\n         Flatten-174                 [-1, 8192]               0\n          Linear-175                 [-1, 2048]      16,779,264\n            ReLU-176                 [-1, 2048]               0\n         Dropout-177                 [-1, 2048]               0\n          Linear-178                  [-1, 512]       1,049,088\n            ReLU-179                  [-1, 512]               0\n         Dropout-180                  [-1, 512]               0\n          Linear-181                    [-1, 2]           1,026\n================================================================\nTotal params: 41,337,410\nTrainable params: 41,337,410\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 1.54\nForward/backward pass size (MB): 772.92\nParams size (MB): 157.69\nEstimated Total Size (MB): 932.15\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import random_split\ntransform1 = transforms.Compose([\n    transforms.Resize((280, 480)),\n    transforms.ToTensor()\n])\ntrain_data_path = '/kaggle/input/bikehorse/Assignment2_BikeHorses'\n\nbikehorsedataset = datasets.ImageFolder(root=train_data_path, transform=transform1)\n\nvalidation_ratio = 0.2  # 20% of the data will be used for validation\n\n# Calculate the sizes of the train and validation sets\ntrain_size = int((1 - validation_ratio) * len(bikehorsedataset))\nval_size = len(bikehorsedataset) - train_size\n\n# Split the dataset into train and validation sets\ntrain_dataset, val_dataset = random_split(bikehorsedataset, [train_size, val_size])\n\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_loader=torch.utils.data.DataLoader(val_dataset,batch_size=4,shuffle=True)\n\ntrain_loader.dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:03:17.236799Z","iopub.execute_input":"2024-03-13T13:03:17.237649Z","iopub.status.idle":"2024-03-13T13:03:17.250575Z","shell.execute_reply.started":"2024-03-13T13:03:17.237620Z","shell.execute_reply":"2024-03-13T13:03:17.249748Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataset.Subset at 0x7ca78d733b50>"},"metadata":{}}]},{"cell_type":"code","source":"epoch_losses1 = []\nnum_epochs = 15\nlearning_rate=0.005\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(bikehorsemodel.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    bikehorsemodel.train()\n    running_loss = 0.0\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n        for images, labels in tepoch:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = bikehorsemodel(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n            tepoch.set_postfix(loss=running_loss / ((tepoch.n + 1) * train_loader.batch_size))\n\n    # Print statistics\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_losses1.append(epoch_loss)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:03:20.384529Z","iopub.execute_input":"2024-03-13T13:03:20.385265Z","iopub.status.idle":"2024-03-13T13:04:12.154396Z","shell.execute_reply.started":"2024-03-13T13:03:20.385230Z","shell.execute_reply":"2024-03-13T13:04:12.153439Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d17745f55f47e7867764eca7eda95d"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/15], Loss: 0.7445\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d5be68ab11468282ecc4d086755c5c"}},"metadata":{}},{"name":"stdout","text":"Epoch [2/15], Loss: 0.3502\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77048d615744efca317633213d8c9e5"}},"metadata":{}},{"name":"stdout","text":"Epoch [3/15], Loss: 0.6159\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf7688ce07a4310b77fbaa04dd771ad"}},"metadata":{}},{"name":"stdout","text":"Epoch [4/15], Loss: 0.4595\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e2c3d719444ab897d603958384fa3c"}},"metadata":{}},{"name":"stdout","text":"Epoch [5/15], Loss: 0.1124\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f2f024ba4d47e597a01f880cecdebe"}},"metadata":{}},{"name":"stdout","text":"Epoch [6/15], Loss: 0.2213\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59c79886619b48159bf203428f89ef11"}},"metadata":{}},{"name":"stdout","text":"Epoch [7/15], Loss: 0.4534\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6d11bb15b7140839d3fc990443c2a1f"}},"metadata":{}},{"name":"stdout","text":"Epoch [8/15], Loss: 0.1984\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e08a63360434277b4cc7330a34f432e"}},"metadata":{}},{"name":"stdout","text":"Epoch [9/15], Loss: 0.9087\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"318eb148517c4e6eb027987b438bb223"}},"metadata":{}},{"name":"stdout","text":"Epoch [10/15], Loss: 0.1130\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8ace3f1f1842cf852c28fe02cdd9b9"}},"metadata":{}},{"name":"stdout","text":"Epoch [11/15], Loss: 0.2160\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c61db81fa642f6ad01b54258c15d18"}},"metadata":{}},{"name":"stdout","text":"Epoch [12/15], Loss: 0.3598\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ff8f9e146f41cf85903b9eafe89a7c"}},"metadata":{}},{"name":"stdout","text":"Epoch [13/15], Loss: 0.1035\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8c477f5d6a43e1bace7e7c5e5addb2"}},"metadata":{}},{"name":"stdout","text":"Epoch [14/15], Loss: 0.1378\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"766ff2c499dd421ea4ce271b24c7822e"}},"metadata":{}},{"name":"stdout","text":"Epoch [15/15], Loss: 0.0777\n","output_type":"stream"}]},{"cell_type":"code","source":"scores = validate(bikehorsemodel,test_loader)\nprint(f\"| Test Loss: {scores[0]: 7.3f}  | Test acc: {scores[1]: 1.5f}  | Test f1: {scores[2]: 1.5f}  |\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T13:04:39.015271Z","iopub.execute_input":"2024-03-13T13:04:39.015937Z","iopub.status.idle":"2024-03-13T13:04:39.421527Z","shell.execute_reply.started":"2024-03-13T13:04:39.015904Z","shell.execute_reply":"2024-03-13T13:04:39.420433Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a28ec547398499ebf65c81b39c639df"}},"metadata":{}},{"name":"stdout","text":"| Test Loss:   0.315  | Test acc:  1.00000  | Test f1:  1.00000  |\n","output_type":"stream"}]}]}